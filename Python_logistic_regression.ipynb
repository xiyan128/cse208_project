{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from concrete.ml.sklearn import LogisticRegression as LR\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, num_points=100):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(num_points, n)\n",
    "    \n",
    "    # Make the criteria for assigning y more non-linear\n",
    "    y = ((np.sum(X[:, :n//2] >= 0.5, axis=1) > n//4) & (np.sum(X[:, :n//2] >= 0.7, axis=1) > n//6)).astype(int)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def save_test_data_to_file(filename, X_test, y_test):\n",
    "    with open(filename, 'w') as file:\n",
    "        for i in range(X_test.shape[0]):\n",
    "            line = ' '.join(map(str, X_test[i])) + ' 1 '+f' {y_test[i]}\\n'\n",
    "            file.write(line)\n",
    "\n",
    "def save_weights_to_file(filename, weights, intercept):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(' '.join(map(str, weights)))\n",
    "        file.write(f'\\n{intercept}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data with 2 dimensions processed.\n",
      "<class 'concrete.ml.sklearn.linear_model.LogisticRegression'> <class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function you are trying to compile cannot be converted to MLIR:\n\n %0 = [[255] [  0]]                         # ClearTensor<uint8, shape=(2, 1)>\n %1 = 40                                    # ClearScalar<uint6>\n %2 = _input_0                              # EncryptedTensor<uint8, shape=(1, 2)>\n %3 = 1                                     # ClearScalar<uint1>\n %4 = add(%2, %3)                           # EncryptedTensor<uint9, shape=(1, 2)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %5 = subgraph(%4)                          # EncryptedTensor<uint8, shape=(1, 2)>\n %6 = matmul(%5, %0)                        # EncryptedTensor<uint16, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %7 = sum(%5, axis=1, keepdims=True)        # EncryptedTensor<uint9, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %8 = multiply(%1, %7)                      # EncryptedTensor<uint15, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %9 = add(%6, %8)                           # EncryptedTensor<uint17, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n%10 = subgraph(%9)                          # EncryptedTensor<uint8, shape=(1, 1)>\nreturn %10\n\nSubgraphs:\n\n    %5 = subgraph(%4):\n\n         %0 = 0                              # ClearScalar<uint1>\n         %1 = 255                            # ClearScalar<uint8>\n         %2 = -1                             # ClearScalar<int1>\n         %3 = 0.0038484894881447643          # ClearScalar<float64>\n         %4 = 0.0038484894881447643          # ClearScalar<float64>\n         %5 = input                          # EncryptedTensor<uint2, shape=(1, 2)>\n         %6 = multiply(%4, %5)               # EncryptedTensor<float64, shape=(1, 2)>\n         %7 = true_divide(%6, %3)            # EncryptedTensor<float64, shape=(1, 2)>\n         %8 = add(%7, %2)                    # EncryptedTensor<float64, shape=(1, 2)>\n         %9 = rint(%8)                       # EncryptedTensor<float64, shape=(1, 2)>\n        %10 = clip(%9, %0, %1)               # EncryptedTensor<float64, shape=(1, 2)>\n        %11 = astype(%10, dtype=int_)        # EncryptedTensor<uint1, shape=(1, 2)>\n        return %11\n\n    %10 = subgraph(%9):\n\n         %0 = 0                                 # ClearScalar<uint1>\n         %1 = 255                               # ClearScalar<uint8>\n         %2 = -23                               # ClearScalar<int6>\n         %3 = 0.0026415093086641184             # ClearScalar<float64>\n         %4 = 1.0                               # ClearScalar<float64>\n         %5 = 1.0                               # ClearScalar<float64>\n         %6 = [-2.8879607]                      # ClearTensor<float32, shape=(1,)>\n         %7 = 4.73919145867945e-05              # ClearScalar<float64>\n         %8 = [[255]]                           # ClearTensor<uint8, shape=(1, 1)>\n         %9 = 80                                # ClearScalar<uint7>\n        %10 = input                             # EncryptedTensor<uint2, shape=(1, 1)>\n        %11 = astype(%10, dtype=float32)        # EncryptedTensor<float32, shape=(1, 1)>\n        %12 = add(%11, %9)                      # EncryptedTensor<float32, shape=(1, 1)>\n        %13 = add(%12, %8)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %14 = multiply(%7, %13)                 # EncryptedTensor<float64, shape=(1, 1)>\n        %15 = add(%14, %6)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %16 = negative(%15)                     # EncryptedTensor<float64, shape=(1, 1)>\n        %17 = exp(%16)                          # EncryptedTensor<float64, shape=(1, 1)>\n        %18 = add(%5, %17)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %19 = true_divide(%4, %18)              # EncryptedTensor<float64, shape=(1, 1)>\n        %20 = true_divide(%19, %3)              # EncryptedTensor<float64, shape=(1, 1)>\n        %21 = add(%20, %2)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %22 = rint(%21)                         # EncryptedTensor<float64, shape=(1, 1)>\n        %23 = clip(%22, %0, %1)                 # EncryptedTensor<float64, shape=(1, 1)>\n        %24 = astype(%23, dtype=int_)           # EncryptedTensor<uint1, shape=(1, 1)>\n        return %24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5c019c407de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# We then compile on a representative set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/concrete/ml/sklearn/base.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, X, configuration, compilation_artifacts, show_mlir, use_virtual_lib, p_error)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         circuit = self.quantized_module_.compile(\n\u001b[0m\u001b[1;32m   1102\u001b[0m             \u001b[0mquantized_numpy_inputset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/concrete/ml/quantization/quantized_module.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, q_inputs, configuration, compilation_artifacts, show_mlir, use_virtual_lib, p_error)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0minputset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputset_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         self.forward_fhe = compiler.compile(\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0minputset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/concrete/numpy/compilation/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, inputset, configuration, artifacts, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mmlir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mlir_to_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/concrete/numpy/mlir/graph_converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(graph, virtual)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Virtual circuits doesn't have MLIR.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mGraphConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_bit_widths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mGraphConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset_negative_lookup_table_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/concrete/numpy/mlir/graph_converter.py\u001b[0m in \u001b[0;36m_update_bit_widths\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffending_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0;34m\"Function you are trying to compile cannot be converted to MLIR:\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhighlighted_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffending_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function you are trying to compile cannot be converted to MLIR:\n\n %0 = [[255] [  0]]                         # ClearTensor<uint8, shape=(2, 1)>\n %1 = 40                                    # ClearScalar<uint6>\n %2 = _input_0                              # EncryptedTensor<uint8, shape=(1, 2)>\n %3 = 1                                     # ClearScalar<uint1>\n %4 = add(%2, %3)                           # EncryptedTensor<uint9, shape=(1, 2)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %5 = subgraph(%4)                          # EncryptedTensor<uint8, shape=(1, 2)>\n %6 = matmul(%5, %0)                        # EncryptedTensor<uint16, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %7 = sum(%5, axis=1, keepdims=True)        # EncryptedTensor<uint9, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %8 = multiply(%1, %7)                      # EncryptedTensor<uint15, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n %9 = add(%6, %8)                           # EncryptedTensor<uint17, shape=(1, 1)>\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ only up to 8-bit integers are supported\n%10 = subgraph(%9)                          # EncryptedTensor<uint8, shape=(1, 1)>\nreturn %10\n\nSubgraphs:\n\n    %5 = subgraph(%4):\n\n         %0 = 0                              # ClearScalar<uint1>\n         %1 = 255                            # ClearScalar<uint8>\n         %2 = -1                             # ClearScalar<int1>\n         %3 = 0.0038484894881447643          # ClearScalar<float64>\n         %4 = 0.0038484894881447643          # ClearScalar<float64>\n         %5 = input                          # EncryptedTensor<uint2, shape=(1, 2)>\n         %6 = multiply(%4, %5)               # EncryptedTensor<float64, shape=(1, 2)>\n         %7 = true_divide(%6, %3)            # EncryptedTensor<float64, shape=(1, 2)>\n         %8 = add(%7, %2)                    # EncryptedTensor<float64, shape=(1, 2)>\n         %9 = rint(%8)                       # EncryptedTensor<float64, shape=(1, 2)>\n        %10 = clip(%9, %0, %1)               # EncryptedTensor<float64, shape=(1, 2)>\n        %11 = astype(%10, dtype=int_)        # EncryptedTensor<uint1, shape=(1, 2)>\n        return %11\n\n    %10 = subgraph(%9):\n\n         %0 = 0                                 # ClearScalar<uint1>\n         %1 = 255                               # ClearScalar<uint8>\n         %2 = -23                               # ClearScalar<int6>\n         %3 = 0.0026415093086641184             # ClearScalar<float64>\n         %4 = 1.0                               # ClearScalar<float64>\n         %5 = 1.0                               # ClearScalar<float64>\n         %6 = [-2.8879607]                      # ClearTensor<float32, shape=(1,)>\n         %7 = 4.73919145867945e-05              # ClearScalar<float64>\n         %8 = [[255]]                           # ClearTensor<uint8, shape=(1, 1)>\n         %9 = 80                                # ClearScalar<uint7>\n        %10 = input                             # EncryptedTensor<uint2, shape=(1, 1)>\n        %11 = astype(%10, dtype=float32)        # EncryptedTensor<float32, shape=(1, 1)>\n        %12 = add(%11, %9)                      # EncryptedTensor<float32, shape=(1, 1)>\n        %13 = add(%12, %8)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %14 = multiply(%7, %13)                 # EncryptedTensor<float64, shape=(1, 1)>\n        %15 = add(%14, %6)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %16 = negative(%15)                     # EncryptedTensor<float64, shape=(1, 1)>\n        %17 = exp(%16)                          # EncryptedTensor<float64, shape=(1, 1)>\n        %18 = add(%5, %17)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %19 = true_divide(%4, %18)              # EncryptedTensor<float64, shape=(1, 1)>\n        %20 = true_divide(%19, %3)              # EncryptedTensor<float64, shape=(1, 1)>\n        %21 = add(%20, %2)                      # EncryptedTensor<float64, shape=(1, 1)>\n        %22 = rint(%21)                         # EncryptedTensor<float64, shape=(1, 1)>\n        %23 = clip(%22, %0, %1)                 # EncryptedTensor<float64, shape=(1, 1)>\n        %24 = astype(%23, dtype=int_)           # EncryptedTensor<uint1, shape=(1, 1)>\n        return %24"
     ]
    }
   ],
   "source": [
    "dimensions = [2, 4, 8, 16, 32]\n",
    "\n",
    "for n in dimensions:\n",
    "    X, y = generate_data(n)\n",
    "\n",
    "    # Split the dataset into training and test sets (70% train, 30% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train a logistic regression classifier\n",
    "    clf = LogisticRegression(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Test the classifier on the test set and measure prediction time\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Save test data to a file\n",
    "    test_data_filename = f'test_data_{n}D.txt'\n",
    "    save_test_data_to_file(test_data_filename, X_test, y_test)\n",
    "    \n",
    "    # Save weights to a file\n",
    "    weights_filename = f'weights_{n}D.txt'\n",
    "    save_weights_to_file(weights_filename, clf.coef_[0], clf.intercept_[0])\n",
    "\n",
    "    result_filename = f'results_{n}D.txt'\n",
    "    with open(result_filename, 'w') as file:\n",
    "        file.write(f'Test Accuracy: {test_accuracy:.4f}\\n')\n",
    "        file.write(f'Time to predict on test data: {prediction_time:.6f} seconds\\n')\n",
    "\n",
    "    print(f'Test data with {n} dimensions processed.')\n",
    "\n",
    "    model = LR(n_bits=8)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # We can simulate the predictions in the clear\n",
    "    y_pred_clear = model.predict(X_test)\n",
    "    print(type(model), type(clf))\n",
    "    # We then compile on a representative set \n",
    "    model.compile(X_train)\n",
    "    \n",
    "\n",
    "    # Finally we run the inference on encrypted inputs !\n",
    "    y_pred_fhe = model.predict(X_test, fhe=\"execute\")\n",
    "    test_accuracy_fhe = accuracy_score(y_test, y_pred_fhe)\n",
    "    test_accuracy_clear_concrete = accuracy_score(y_test, y_pred_clear)\n",
    "\n",
    "    print(\"Accuracy_clear_concrete  :\", test_accuracy_clear_concrete)\n",
    "    print(\"Accuracy_FHE    :\", test_accuracy_fhe)\n",
    "    print(f\"Similarity: {int((y_pred_fhe == y_pred_clear).mean()*100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.63872993 2.35402328 0.14803324 0.0739867 ] -3.9148855975417045 [[0.01439349 0.11607264 0.04600264 0.0407288 ]\n",
      " [0.65196126 0.22426931 0.71217922 0.23724909]\n",
      " [0.89000534 0.33799516 0.37558295 0.09398194]\n",
      " [0.34106635 0.11347352 0.92469362 0.87733935]\n",
      " [0.69093774 0.38673535 0.93672999 0.13752094]\n",
      " [0.67213555 0.76161962 0.23763754 0.72821635]\n",
      " [0.88721274 0.47221493 0.11959425 0.71324479]\n",
      " [0.54922666 0.71459592 0.66019738 0.2799339 ]\n",
      " [0.12203823 0.49517691 0.03438852 0.9093204 ]\n",
      " [0.37454012 0.95071431 0.73199394 0.59865848]\n",
      " [0.00552212 0.81546143 0.70685734 0.72900717]\n",
      " [0.80744016 0.8960913  0.31800347 0.11005192]\n",
      " [0.82260056 0.36019064 0.12706051 0.52224326]\n",
      " [0.11986537 0.33761517 0.9429097  0.32320293]\n",
      " [0.38816993 0.64328822 0.45825289 0.54561679]\n",
      " [0.30424224 0.52475643 0.43194502 0.29122914]\n",
      " [0.72609133 0.97585208 0.51630035 0.32295647]\n",
      " [0.79518619 0.27083225 0.43897142 0.07845638]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.22793516 0.42710779 0.81801477 0.86073058]\n",
      " [0.65761289 0.5683086  0.09367477 0.3677158 ]\n",
      " [0.65869363 0.16293443 0.07056875 0.64241928]\n",
      " [0.90756647 0.24929223 0.41038292 0.75555114]\n",
      " [0.04077514 0.59089294 0.67756436 0.01658783]\n",
      " [0.79829518 0.64996393 0.70196688 0.79579267]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]\n",
      " [0.36778313 0.63230583 0.63352971 0.53577468]\n",
      " [0.73507104 0.80348093 0.28203457 0.17743954]\n",
      " [0.30461377 0.09767211 0.68423303 0.44015249]\n",
      " [0.28654125 0.59083326 0.03050025 0.03734819]]\n",
      "   Dot Product: -3.593844061182282, Logistic Output: 0.026756833638048014\n",
      "   Dot Product: -1.5436212611116495, Logistic Output: 0.17600946646192242\n",
      "   Dot Product: -0.7082012231487034, Logistic Output: 0.32999642587874756\n",
      "   Dot Product: -2.5459874677458734, Logistic Output: 0.07269651376042247\n",
      "   Dot Product: -1.0324616063932175, Logistic Output: 0.26260714882051445\n",
      "   Dot Product: -0.2593745325993404, Logistic Output: 0.43551746814919773\n",
      "   Dot Product: -0.39169130546340725, Logistic Output: 0.4033102197323148\n",
      "   Dot Product: -0.6650067834584883, Logistic Output: 0.3396158055460187\n",
      "   Dot Product: -2.35483342661699, Logistic Output: 0.0866823519714879\n",
      "   Dot Product: -0.5359195722096266, Logistic Output: 0.36913730325459226\n",
      "   Dot Product: -1.8221238248106801, Logistic Output: 0.13917922592961843\n",
      "   Dot Product: 0.3803681474388392, Logistic Output: 0.5939618925504229\n",
      "   Dot Product: -0.8389194913663878, Logistic Output: 0.3017624003929427\n",
      "   Dot Product: -2.6403445987190324, Logistic Output: 0.06658661458786548\n",
      "   Dot Product: -1.2680895126801084, Logistic Output: 0.2195844714184824\n",
      "   Dot Product: -1.7912943285492235, Logistic Output: 0.14291410832080337\n",
      "   Dot Product: 0.39857594147103503, Logistic Output: 0.5983454667152975\n",
      "   Dot Product: -1.1082714720353342, Logistic Output: 0.24819328002572597\n",
      "   Dot Product: -1.8362328601926792, Logistic Output: 0.13749743502778417\n",
      "   Dot Product: -2.123228599240293, Logistic Output: 0.10685953975933662\n",
      "   Dot Product: -0.8007380391760419, Logistic Output: 0.30986766727553844\n",
      "   Dot Product: -1.7352425633660502, Logistic Output: 0.14991822609613065\n",
      "   Dot Product: -0.8165720237690599, Logistic Output: 0.30649180821497196\n",
      "   Dot Product: -2.3147859430967617, Logistic Output: 0.08990577643625099\n",
      "   Dot Product: -0.11557749334134026, Logistic Output: 0.4711377483878518\n",
      "   Dot Product: -2.1014335656672767, Logistic Output: 0.10895756426516609\n",
      "   Dot Product: -1.3225189408715359, Logistic Output: 0.21039951263342022\n",
      "   Dot Product: -0.028940164169000582, Logistic Output: 0.49276546388099157\n",
      "   Dot Product: -2.7473150382029212, Logistic Output: 0.060238465959414664\n",
      "   Dot Product: -1.760667051477502, Logistic Output: 0.1467068159979743\n",
      "For n=4:\n",
      "   Minimum Dot Product: -3.593844061182282\n",
      "   Maximum Dot Product: 0.39857594147103503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_weights(filename):\n",
    "    # Read weights from file\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract weights (excluding the intercept)\n",
    "    weights = np.loadtxt(lines[:-1])\n",
    "\n",
    "    # Extract intercept (last line)\n",
    "    intercept = float(lines[-1])\n",
    "\n",
    "    return weights, intercept\n",
    "\n",
    "def read_test_data(filename, n):\n",
    "    # Read test data from file\n",
    "    with open(filename, 'r') as file:\n",
    "        data = np.loadtxt(file, usecols=range(n))\n",
    "    return data\n",
    "\n",
    "def calculate_dot_products(weights, intercept, data):\n",
    "    # Calculate dot products for each row of data\n",
    "    print(weights, intercept, data)\n",
    "    dot_products = np.dot(data, weights)+intercept\n",
    "    return dot_products\n",
    "\n",
    "def logistic_function(x):\n",
    "    # Calculate logistic function 1 / (1 + exp(-x))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def find_min_max_dot_products(weights_filename, test_data_filename):\n",
    "    # Read weights for different n values\n",
    "    n_values = [4]\n",
    "    min_dot_products = []\n",
    "    max_dot_products = []\n",
    "\n",
    "    for n in n_values:\n",
    "        # Construct filenames\n",
    "        weights_filename_n = weights_filename.replace('$n', str(n))\n",
    "        test_data_filename_n = test_data_filename.replace('$n', str(n))\n",
    "\n",
    "        # Read weights and intercept\n",
    "        weights, intercept = read_weights(weights_filename_n)\n",
    "        data = read_test_data(test_data_filename_n, n)\n",
    "\n",
    "        # Calculate dot products\n",
    "        dot_products = calculate_dot_products(weights, intercept, data)\n",
    "\n",
    "        # Find minimum and maximum dot products\n",
    "        min_dot_products.append(np.min(dot_products))\n",
    "        max_dot_products.append(np.max(dot_products))\n",
    "        \n",
    "        for dot_product in dot_products:\n",
    "            logistic_output = logistic_function(dot_product)\n",
    "            print(f\"   Dot Product: {dot_product}, Logistic Output: {logistic_output}\")\n",
    "\n",
    "    return min_dot_products, max_dot_products\n",
    "\n",
    "# Example usage\n",
    "weights_filename = \"weights_$nD.txt\"\n",
    "test_data_filename = \"test_data_$nD.txt\"\n",
    "\n",
    "min_dot_products, max_dot_products = find_min_max_dot_products(weights_filename, test_data_filename)\n",
    "\n",
    "# Print results\n",
    "for i, n in enumerate([4]):\n",
    "    print(f\"For n={n}:\")\n",
    "    print(f\"   Minimum Dot Product: {min_dot_products[i]}\")\n",
    "    print(f\"   Maximum Dot Product: {max_dot_products[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
